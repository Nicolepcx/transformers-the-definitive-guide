{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Open notebook in:\n",
        "| Colab                                                                                                 \n",
        "|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nicolepcx/transformers-the-definitive-guide/blob/master/CH07/ch07_replay_buffer_DT.ipynb)                                             "
      ],
      "metadata": {
        "id": "UnrBeLs8Rz-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About This Section: Replay Buffer Implementation\n",
        "\n",
        "This section of the notebook focuses on the implementation of a **Replay Buffer** specifically designed for the [Online Decision Transformer](https://arxiv.org/abs/2202.05607) (ODT) framework. The replay buffer is a critical component in reinforcement learning, particularly for algorithms like ODT that require the storage and efficient retrieval of past experiences.\n",
        "\n",
        "#### Key Features:\n",
        "- **Capacity Management**: The replay buffer is initialized with a specified capacity, ensuring that only the top trajectories (based on rewards) are retained when the buffer is full.\n",
        "- **Trajectory Storage**: Unlike traditional replay buffers that store individual transitions, this implementation stores entire trajectories, which are sequences of states, actions, and rewards. This approach aligns with ODT's focus on sequence-level modeling.\n",
        "- **FIFO Replacement Strategy**: As new trajectories are collected during the training process, the replay buffer replaces the oldest trajectories in a first-in-first-out (FIFO) manner, maintaining a dynamic and up-to-date set of experiences for the agent to learn from.\n",
        "- **Efficiency**: The replay buffer enhances the learning process by allowing the model to continuously learn from a diverse set of past experiences, improving stability and generalization.\n",
        "\n",
        "This replay buffer implementation is integral to the ODT's ability to adapt its policy based on new data while retaining valuable information from previous experiences.\n"
      ],
      "metadata": {
        "id": "3igahluq2HOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "JHuhhH-8qur5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "v7LT02pIqyNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Buffer"
      ],
      "metadata": {
        "id": "lV3bQAefq9H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, capacity, trajectories=[]):\n",
        "        self.capacity = capacity\n",
        "        if len(trajectories) <= self.capacity:\n",
        "            self.trajectories = trajectories\n",
        "        else:\n",
        "            returns = [traj[\"rewards\"].sum() for traj in trajectories]\n",
        "            sorted_inds = np.argsort(returns)  # lowest to highest\n",
        "            self.trajectories = [\n",
        "                trajectories[ii] for ii in sorted_inds[-self.capacity :]\n",
        "            ]\n",
        "\n",
        "        self.start_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trajectories)\n",
        "\n",
        "    def add_new_trajs(self, new_trajs):\n",
        "        if len(self.trajectories) < self.capacity:\n",
        "            self.trajectories.extend(new_trajs)\n",
        "            self.trajectories = self.trajectories[-self.capacity :]\n",
        "        else:\n",
        "            self.trajectories[\n",
        "                self.start_idx : self.start_idx + len(new_trajs)\n",
        "            ] = new_trajs\n",
        "            self.start_idx = (self.start_idx + len(new_trajs)) % self.capacity\n",
        "\n",
        "        assert len(self.trajectories) <= self.capacity"
      ],
      "metadata": {
        "id": "n0FzJNI6jJ8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run an Example of Trajectories"
      ],
      "metadata": {
        "id": "rpnDwbsxq0pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example trajectory data\n",
        "trajectories = [\n",
        "    {\"rewards\": np.array([1, 2, 3]), \"states\": np.array([0, 1, 2]), \"actions\": np.array([0, 1, 2])},\n",
        "    {\"rewards\": np.array([2, 3, 4]), \"states\": np.array([1, 2, 3]), \"actions\": np.array([1, 2, 3])},\n",
        "    {\"rewards\": np.array([3, 4, 5]), \"states\": np.array([2, 3, 4]), \"actions\": np.array([2, 3, 4])},\n",
        "    {\"rewards\": np.array([4, 5, 6]), \"states\": np.array([3, 4, 5]), \"actions\": np.array([3, 4, 5])}\n",
        "]\n",
        "\n",
        "# Initialize the replay buffer with a capacity of 3\n",
        "replay_buffer = ReplayBuffer(capacity=3, trajectories=trajectories[:2])\n",
        "\n",
        "# Print the initial state of the replay buffer\n",
        "print(\"Initial replay buffer:\")\n",
        "for traj in replay_buffer.trajectories:\n",
        "    print(traj)\n",
        "\n",
        "# Add new trajectories to the buffer\n",
        "new_trajectories = [\n",
        "    {\"rewards\": np.array([5, 6, 7]), \"states\": np.array([4, 5, 6]), \"actions\": np.array([4, 5, 6])},\n",
        "    {\"rewards\": np.array([6, 7, 8]), \"states\": np.array([5, 6, 7]), \"actions\": np.array([5, 6, 7])}\n",
        "]\n",
        "replay_buffer.add_new_trajs(new_trajectories)\n",
        "\n",
        "# Print the state of the replay buffer after adding new trajectories\n",
        "print(\"\\nReplay buffer after adding new trajectories:\")\n",
        "for traj in replay_buffer.trajectories:\n",
        "    print(traj)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II5n_lMvprFr",
        "outputId": "107f17c6-a52e-4bbb-fdfc-f2dc3885b30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial replay buffer:\n",
            "{'rewards': array([1, 2, 3]), 'states': array([0, 1, 2]), 'actions': array([0, 1, 2])}\n",
            "{'rewards': array([2, 3, 4]), 'states': array([1, 2, 3]), 'actions': array([1, 2, 3])}\n",
            "\n",
            "Replay buffer after adding new trajectories:\n",
            "{'rewards': array([2, 3, 4]), 'states': array([1, 2, 3]), 'actions': array([1, 2, 3])}\n",
            "{'rewards': array([5, 6, 7]), 'states': array([4, 5, 6]), 'actions': array([4, 5, 6])}\n",
            "{'rewards': array([6, 7, 8]), 'states': array([5, 6, 7]), 'actions': array([5, 6, 7])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CyGqisJrpvh4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}