{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Open notebook in:\n",
        "| Colab                                 |  \n",
        "|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nicolepcx/transformers-the-definitive-guide/blob/master/CH04/ch04_KV_compression.ipynb)                                                        "
      ],
      "metadata": {
        "id": "AkqeISGNBj95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this notebook\n",
        "\n",
        "\n",
        "In this notebook, you explore an optimized approach to handle high-resolution image generation using a key-value token compression mechanism within attention modules. The `AttentionKVCompress` class demonstrates how to effectively downsample tensors, thereby reducing computational load while maintaining model performance. By implementing various sampling methods such as convolutional downsampling, average pooling, and uniform token selection, we achieve significant reductions in the number of tokens processed during training and inference. This technique is pivotal in enabling efficient scaling to ultra-high resolutions without compromising on quality. This method was introduced in the paper: [\"PixArt-Î£: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation\"](https://arxiv.org/abs/2403.04692)"
      ],
      "metadata": {
        "id": "tbX2Jhn-bZlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "yd4eDZiWq5-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "PsKPDRT7q9Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionKVCompress(nn.Module):\n",
        "    def __init__(self, dim, sr_ratio):\n",
        "        super(ExampleModel, self).__init__()\n",
        "        self.sr = nn.Conv2d(dim, dim, groups=dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def downsample_2d(self, tensor, H, W, scale_factor, sampling=None):\n",
        "        if sampling is None or scale_factor == 1:\n",
        "            return tensor, tensor.shape[1]\n",
        "\n",
        "        B, N, C = tensor.shape\n",
        "        print(f\"Original Tensor Shape: {tensor.shape}\")\n",
        "\n",
        "        if sampling == 'uniform_every':\n",
        "            tensor = tensor[:, ::scale_factor]\n",
        "            print(f\"Shape after 'uniform_every': {tensor.shape}\")\n",
        "            return tensor, int(N // scale_factor)\n",
        "\n",
        "        tensor = tensor.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
        "        print(f\"Shape after Reshape and Permute: {tensor.shape}\")\n",
        "        new_H, new_W = int(H / scale_factor), int(W / scale_factor)\n",
        "        new_N = new_H * new_W\n",
        "\n",
        "        if sampling == 'ave':\n",
        "            tensor = F.interpolate(\n",
        "                tensor, scale_factor=1 / scale_factor, mode='nearest'\n",
        "            ).permute(0, 2, 3, 1)\n",
        "        elif sampling == 'uniform':\n",
        "            tensor = tensor[:, :, ::scale_factor, ::scale_factor].permute(0, 2, 3, 1)\n",
        "            print(f\"Shape after 'uniform' downsampling: {tensor.shape}\")\n",
        "        elif sampling == 'conv':\n",
        "            tensor = self.sr(tensor).reshape(B, C, -1).permute(0, 2, 1)\n",
        "            tensor = self.norm(tensor)\n",
        "            print(f\"Shape after 'conv' downsampling: {tensor.shape}\")\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        tensor = tensor.reshape(B, new_N, C).contiguous()\n",
        "        print(f\"Final Shape after Reshape: {tensor.shape}\")\n",
        "        return tensor, new_N\n",
        "\n",
        "\n",
        "# Create example tensor\n",
        "B, H, W, C = 1, 4, 4, 1  # Batch size, Height, Width, Channels\n",
        "input_tensor = torch.arange(1, 17).view(B, H * W, C).float()  # Creating a tensor [1, 16, 1]\n",
        "\n",
        "compressed_KV = AttentionKVCompress(dim=C, sr_ratio=2)\n",
        "\n",
        "# Uniform Every\n",
        "print(\"=== Uniform Every Downsampling ===\")\n",
        "output_tensor, new_N = model.downsample_2d(input_tensor, H, W, scale_factor=2, sampling='uniform_every')\n",
        "print(f\"New number of tokens: {new_N}\\n\")\n",
        "\n",
        "# Average Pooling\n",
        "print(\"=== Average Pooling Downsampling ===\")\n",
        "output_tensor, new_N = model.downsample_2d(input_tensor, H, W, scale_factor=2, sampling='ave')\n",
        "print(f\"New number of tokens: {new_N}\\n\")\n",
        "\n",
        "# Uniform\n",
        "print(\"=== Uniform Downsampling ===\")\n",
        "output_tensor, new_N = model.downsample_2d(input_tensor, H, W, scale_factor=2, sampling='uniform')\n",
        "print(f\"New number of tokens: {new_N}\\n\")\n",
        "\n",
        "# Convolution\n",
        "print(\"=== Convolution Downsampling ===\")\n",
        "output_tensor, new_N = model.downsample_2d(input_tensor, H, W, scale_factor=2, sampling='conv')\n",
        "print(f\"New number of tokens: {new_N}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh2ic_uE8web",
        "outputId": "cc5b33bc-160d-438c-94ac-c86db1831f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Uniform Every Downsampling ===\n",
            "Original Tensor Shape: torch.Size([1, 16, 1])\n",
            "Shape after 'uniform_every': torch.Size([1, 8, 1])\n",
            "New number of tokens: 8\n",
            "\n",
            "=== Average Pooling Downsampling ===\n",
            "Original Tensor Shape: torch.Size([1, 16, 1])\n",
            "Shape after Reshape and Permute: torch.Size([1, 1, 4, 4])\n",
            "Final Shape after Reshape: torch.Size([1, 4, 1])\n",
            "New number of tokens: 4\n",
            "\n",
            "=== Uniform Downsampling ===\n",
            "Original Tensor Shape: torch.Size([1, 16, 1])\n",
            "Shape after Reshape and Permute: torch.Size([1, 1, 4, 4])\n",
            "Shape after 'uniform' downsampling: torch.Size([1, 2, 2, 1])\n",
            "Final Shape after Reshape: torch.Size([1, 4, 1])\n",
            "New number of tokens: 4\n",
            "\n",
            "=== Convolution Downsampling ===\n",
            "Original Tensor Shape: torch.Size([1, 16, 1])\n",
            "Shape after Reshape and Permute: torch.Size([1, 1, 4, 4])\n",
            "Shape after 'conv' downsampling: torch.Size([1, 4, 1])\n",
            "Final Shape after Reshape: torch.Size([1, 4, 1])\n",
            "New number of tokens: 4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAJjTNly-FwC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}