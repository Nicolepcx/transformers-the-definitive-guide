{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Open notebook in:\n",
        "| Colab                                  Gradient                                                                        \n",
        ":-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nicolepcx/transformers-the-definitive-guide/blob/master/CH03/ch03_Fine_tune_SAM_with_wandb_optuna.ipynb)                                             "
      ],
      "metadata": {
        "id": "jZI6Z0Nu9iIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About This Notebook\n",
        "\n",
        "This notebook demonstrates a workflow for fine-tuning, training and evaluating a segmentation model using [SAM](https://arxiv.org/abs/2304.02643) (Segment Anything Model) in the context of medical image analysis, specifically for breast cancer datasets.\n",
        "\n",
        "You first will be installing necessary libraries such as `transformers`, `datasets`, `monai`, and `optuna`. Then you load a breast cancer dataset from Hugging Face's `datasets` library and visualize sample images along with their corresponding ground truth segmentation masks.\n",
        "\n",
        "Next, you prepare the dataset for training by defining a PyTorch dataset class (`SAMDataset`) that incorporates SAM's required prompts (bounding boxes derived from segmentation masks). You also set up a PyTorch DataLoader to handle batching during training.\n",
        "\n",
        "The SAM model is loaded from the `transformers` library, and you fine-tune it using a combination of hyperparameter optimization (with [`optuna`](https://optuna.org/)) and logging via [`wandb`](https://wandb.ai/site) to monitor the training process. The training focuses on adjusting the mask decoder parameters of the SAM model, while keeping the vision and prompt encoders frozen.\n",
        "\n",
        "Finally, the notebook demonstrates inference on unseen data, comparing the predicted segmentation masks to ground truth masks.\n",
        "\n",
        "Notebook is inspired by: [SAM inference notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Run_inference_with_MedSAM_using_HuggingFace_Transformers.ipynb).\n",
        "\n"
      ],
      "metadata": {
        "id": "t5VJLg7z9qBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "_OtZlZNz-dS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets==2.21.0 transformers==4.42.4 accelerate==0.32.1 evaluate==0.4.2 monai==1.3.2 optuna==3.6.1 wandb==0.17.7 -qqq"
      ],
      "metadata": {
        "id": "ChwVUi1iqiqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068db0e6-d93e-4e6d-fcdc-914c075044b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/527.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/380.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m7.0/9.3 MB\u001b[0m \u001b[31m210.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m181.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/233.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "80wkeeqR-2DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optuna\n",
        "import torch\n",
        "from PIL import Image\n",
        "from statistics import mean\n",
        "from tqdm import tqdm\n",
        "\n",
        "import monai\n",
        "import wandb\n",
        "from datasets import load_dataset\n",
        "from torch.nn.functional import normalize, threshold\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import SamModel, SamProcessor\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-1rOzeR2-8bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset and Show Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "ROd15m4Ucdut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to display a image mask."
      ],
      "metadata": {
        "id": "hrg4UIq6WE9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    \"\"\"\n",
        "    Display a mask on a given matplotlib axis.\n",
        "\n",
        "    Args:\n",
        "        mask (numpy.ndarray): The mask to be displayed, typically a binary mask.\n",
        "        ax (matplotlib.axes.Axes): The axis on which to display the mask.\n",
        "        random_color (bool, optional): If True, the mask will be displayed with a random color.\n",
        "                                       If False, a default color (light blue) will be used.\n",
        "                                       Defaults to False.\n",
        "    \"\"\"\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)"
      ],
      "metadata": {
        "id": "da4FtmowWBpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"nielsr/breast-cancer\", split=\"train\")"
      ],
      "metadata": {
        "id": "kRf-WnHqcbcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "kJ58CT8wdsPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Image"
      ],
      "metadata": {
        "id": "sMXarSi9_CAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset[10]\n",
        "image = example[\"image\"]\n"
      ],
      "metadata": {
        "id": "uTchd20g1Nri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image);"
      ],
      "metadata": {
        "id": "8suvbiP357vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show Ground Truth Mask"
      ],
      "metadata": {
        "id": "_zsd27OC_Sxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots()\n",
        "axes.imshow(np.array(image))\n",
        "ground_truth_seg = np.array(example[\"label\"])\n",
        "show_mask(ground_truth_seg, axes)\n",
        "axes.title.set_text(f\"Ground truth mask\")\n",
        "axes.axis(\"off\")"
      ],
      "metadata": {
        "id": "nXqmv5c58p4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing the PyTorch Dataset\n",
        "\n",
        "Each dataset entry is composed of:\n",
        "\n",
        "* Pixel values\n",
        "* A prompt in the form of a bounding box\n",
        "* A ground truth segmentation mask\n",
        "\n",
        "The following function outlines how to generate a bounding box prompt based on the ground truth segmentation mask.\n",
        "\n",
        "SAM is trained using \"prompts,\" which can take the form of bounding boxes, points, text, or basic masks. The model is then trained to produce the correct mask given the image and the corresponding prompt.\n"
      ],
      "metadata": {
        "id": "J2cOa80Qc3FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bounding_box(ground_truth_map):\n",
        "    \"\"\"\n",
        "    Calculate a bounding box around the non-zero regions of a ground truth map with some random perturbation.\n",
        "\n",
        "    Args:\n",
        "        ground_truth_map (numpy.ndarray): A 2D array representing the binary ground truth map where\n",
        "                                          non-zero values indicate the region of interest.\n",
        "\n",
        "    Returns:\n",
        "        list: A list representing the bounding box coordinates [x_min, y_min, x_max, y_max].\n",
        "    \"\"\"\n",
        "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
        "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "\n",
        "    H, W = ground_truth_map.shape\n",
        "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
        "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
        "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
        "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
        "    bbox = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "    return bbox"
      ],
      "metadata": {
        "id": "bdJTJ9jsV103"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAMDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class for processing images and masks using a SAM (Segment Anything Model) processor.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): A dataset object containing image data and corresponding labels.\n",
        "        processor (SamProcessor): A processor object that prepares images and prompts for the SAM model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of items in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves an item from the dataset at the specified index and processes it for the SAM model.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the item to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing processed inputs and the ground truth mask.\n",
        "        \"\"\"\n",
        "        item = self.dataset[idx]\n",
        "        image = item[\"image\"]\n",
        "        ground_truth_mask = np.array(item[\"label\"])\n",
        "\n",
        "        prompt = get_bounding_box(ground_truth_mask)\n",
        "\n",
        "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
        "\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "\n",
        "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
        "\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "9eI8mMh0VtEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
      ],
      "metadata": {
        "id": "wE4iTOZdeLjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SAMDataset(dataset=dataset, processor=processor)"
      ],
      "metadata": {
        "id": "XTynfgToe8jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = train_dataset[0]\n",
        "for k,v in example.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "id": "dYZKU4iBfB-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "9CUnLOjSf9Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "id": "5D2bmjAhgIus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"ground_truth_mask\"].shape"
      ],
      "metadata": {
        "id": "eJyuJc7fOldT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model"
      ],
      "metadata": {
        "id": "AQx2Aq7LeAMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
        "\n",
        "# Ensure gradients are only computed for the mask decoder\n",
        "for param_name, parameter in model.named_parameters():\n",
        "    if param_name.startswith(\"vision_encoder\") or param_name.startswith(\"prompt_encoder\"):\n",
        "        parameter.requires_grad = False\n"
      ],
      "metadata": {
        "id": "ZI2ioeS5eAxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune the Model"
      ],
      "metadata": {
        "id": "CShhxC-heDpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to Weights & Biases"
      ],
      "metadata": {
        "id": "Ecz8GQi2WYJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.init(project='image segmentation')\n"
      ],
      "metadata": {
        "id": "NzQFwOmInlln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare and Run Optuna Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "CQINORLxWe-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Objective function for Optuna hyperparameter optimization.\n",
        "\n",
        "    This function defines the hyperparameter search space, initializes the model,\n",
        "    optimizer, and loss function, and trains the model for a specified number of epochs.\n",
        "    It reports the mean loss for each epoch and allows Optuna to prune trials that are\n",
        "    not promising.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): A trial object that suggests hyperparameters and\n",
        "                                    manages the state of the optimization process.\n",
        "\n",
        "    Returns:\n",
        "        float: The mean loss over the final epoch of training, which Optuna aims to minimize.\n",
        "    \"\"\"\n",
        "    # Suggested values for the hyperparameters\n",
        "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 0, 1e-3)\n",
        "    num_epochs = trial.suggest_int(\"num_epochs\", 10, 50)\n",
        "    sigmoid = trial.suggest_categorical(\"sigmoid\", [True, False])\n",
        "    squared_pred = trial.suggest_categorical(\"squared_pred\", [True, False])\n",
        "\n",
        "    # Set up the model, optimizer, and loss function\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    seg_loss = monai.losses.DiceFocalLoss(sigmoid=sigmoid, squared_pred=squared_pred, reduction='mean')\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = []\n",
        "        for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "            # Forward and backward passes\n",
        "            outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                            input_boxes=batch[\"input_boxes\"].to(device),\n",
        "                            multimask_output=False)\n",
        "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "            loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "        trial.report(np.mean(epoch_losses), epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(epoch_losses)\n",
        "\n",
        "# Create a study object and specify the direction of the optimization\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=5)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Value: {trial.value}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ],
      "metadata": {
        "id": "HWIeCa6asoJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial.params"
      ],
      "metadata": {
        "id": "Z4PxGnQlvbT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial.params.get(\"lr\")"
      ],
      "metadata": {
        "id": "-qdckvGCuE5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial.params.get(\"sigmoid\")"
      ],
      "metadata": {
        "id": "XPb9I2LjTJpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model on Best Hyperparameter"
      ],
      "metadata": {
        "id": "TpAm48Z8Wslp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Use Hyperparameter from tuning here\n",
        "optimizer = Adam(model.mask_decoder.parameters(), lr=trial.params.get(\"lr\"), weight_decay=trial.params.get(\"weight_decay\"))\n",
        "\n",
        "seg_loss = monai.losses.DiceFocalLoss(sigmoid=trial.params.get(\"sigmoid\"), squared_pred=trial.params.get(\"squared_pred\"), reduction='mean')"
      ],
      "metadata": {
        "id": "vOTrMv1LeEK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = trial.params.get(\"num_epochs\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
        "                        multimask_output=False)\n",
        "\n",
        "        # Compute loss\n",
        "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
        "\n",
        "        # Backward pass (compute gradients of parameters w.r.t. loss)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "        # Log sample images and masks to wandb at a specified interval\n",
        "        if batch_idx % 5 == 0:\n",
        "            # Select the first sample in the batch for logging\n",
        "            image_to_log = batch[\"pixel_values\"][0].permute(1, 2, 0).cpu().numpy()  # Convert to HWC format for wandb\n",
        "            predicted_mask_to_log = predicted_masks[0].cpu().detach().numpy()\n",
        "            ground_truth_mask_to_log = ground_truth_masks[0].cpu().detach().numpy()\n",
        "\n",
        "            # Log using wandb\n",
        "            wandb.log({\n",
        "                \"Input Image\": wandb.Image(image_to_log, caption=\"Input Image\"),\n",
        "                \"Predicted Mask\": wandb.Image(predicted_mask_to_log, caption=\"Predicted Mask\"),\n",
        "                \"Ground Truth Mask\": wandb.Image(ground_truth_mask_to_log, caption=\"Ground Truth Mask\")\n",
        "            }, commit=False)  # Use commit=False to accumulate logs\n",
        "\n",
        "    # Log mean loss for the epoch outside the inner loop\n",
        "    wandb.log({'epoch': epoch, 'mean_loss': mean(epoch_losses)})\n",
        "\n",
        "    print(f'EPOCH: {epoch}')\n",
        "    print(f'Mean loss: {mean(epoch_losses)}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5rB_MeL1qpHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "A key point to remember: since we utilized the Dice loss with `sigmoid=True`, it's essential to correctly apply a sigmoid activation function to the predicted masks. Therefore, we will not be using the processor's `post_process_masks` method in this case."
      ],
      "metadata": {
        "id": "IYKvlFthRsyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take out a random example from the training\n",
        "idx = 10\n",
        "\n",
        "# load image\n",
        "image = dataset[idx][\"image\"]\n",
        "image"
      ],
      "metadata": {
        "id": "nyES0ru3aiVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get box prompt based on ground truth segmentation map\n",
        "ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
        "prompt = get_bounding_box(ground_truth_mask)\n",
        "\n",
        "# Prepare image + box prompt for the model\n",
        "inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
        "for k,v in inputs.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "id": "OzxIzE6janRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs, multimask_output=False)"
      ],
      "metadata": {
        "id": "OvOVzNvea5oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sigmoid\n",
        "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
        "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)"
      ],
      "metadata": {
        "id": "uvgwHg42bACQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "fig, axes = plt.subplots()\n",
        "\n",
        "axes.imshow(np.array(image))\n",
        "show_mask(medsam_seg, axes)\n",
        "axes.title.set_text(f\"Predicted mask\")\n",
        "axes.axis(\"off\")"
      ],
      "metadata": {
        "id": "DCehzlJJbDGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare this to the ground truth segmentation:"
      ],
      "metadata": {
        "id": "WFbiFDAabLNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots()\n",
        "\n",
        "axes.imshow(np.array(image))\n",
        "show_mask(ground_truth_mask, axes)\n",
        "axes.title.set_text(f\"Ground truth mask\")\n",
        "axes.axis(\"off\")"
      ],
      "metadata": {
        "id": "PEISRU0WbEvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FEs58oPbNTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}