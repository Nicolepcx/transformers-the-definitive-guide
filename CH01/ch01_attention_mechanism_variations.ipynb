{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "VWNgh0JCrl6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open notebook in:\n",
        "| Colab                                 |  Gradient                                                                                                                                         |\n",
        "|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/Nicolepcx/transformers-the-definitive-guide/blob/main/CH01/ch01_attention_mechanism_variations.ipynb)                                              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com//github.com/Nicolepcx/transformers-the-definitive-guide/blob/main/CH01/ch01_attention_mechanism_variations.ipynb)|             "
      ],
      "metadata": {
        "id": "Gyg1oxapa9iL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this notebook\n",
        "\n",
        "\n",
        "In this notebook you can run and try out the different attention mechanism variations as introduced about in the book.\n"
      ],
      "metadata": {
        "id": "tbX2Jhn-bZlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjXxXaqr6yMT",
        "outputId": "12d25e6d-5280-4d76-d6b0-3fbf4d0465ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "YmmHZYgUN8vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "EY4XoDBvJonr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head attention"
      ],
      "metadata": {
        "id": "RcqjsW1wOA8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MultiheadAttention(x, M, W_query, W_key, W_value, P_o):\n",
        "    \"\"\"\n",
        "    Multi-head Attention on one query.\n",
        "    Args:\n",
        "    x: a vector with shape [d]\n",
        "    M: a matrix with shape [m, d]\n",
        "    W_query: a tensor with shape [h, d, k]\n",
        "    W_key: a tensor with shape [h, d, k]\n",
        "    W_value: a tensor with shape [h, d, v]\n",
        "    P_o: a tensor with shape [h, d, v]\n",
        "    Returns:\n",
        "    y: a vector with shape [d]\n",
        "    \"\"\"\n",
        "    scaling_factor = W_key.shape[1]**0.5\n",
        "\n",
        "    Q = torch.einsum('d,hdk->hk', x, W_query)\n",
        "    print(\"Q shape: \",Q.shape)\n",
        "    K = torch.einsum('md,hdk->hmk', M, W_key)\n",
        "    print(\"K shape: \",K.shape)\n",
        "    V = torch.einsum('md,hdv->hmv', M, W_value)\n",
        "    print(\"V shape: \",V.shape)\n",
        "    attn_scores = torch.einsum('hk,hmk->hm', Q, K) / scaling_factor\n",
        "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    o = torch.einsum('hm,hmv->hv', attn_weights, V)\n",
        "    y = torch.einsum('hv,hdv->d', o, P_o)\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "Z2R5EICk0vJX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running multi-head attention with parameters"
      ],
      "metadata": {
        "id": "kjCEzn5_OGxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensions for tensors\n",
        "d, m, h, k, v = 4, 3, 2, 5, 5\n",
        "\n",
        "# Input tensors\n",
        "x = torch.randn(d)\n",
        "M = torch.randn(m, d)\n",
        "W_query = torch.randn(h, d, k)\n",
        "W_key = torch.randn(h, d, k)\n",
        "W_value = torch.randn(h, d, v)\n",
        "P_o = torch.randn(h, d, v)\n",
        "\n",
        "# Call the function\n",
        "y = MultiheadAttention(x, M, W_query, W_key, W_value, P_o)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6G-jxXBRP1D",
        "outputId": "b8b90fee-03f7-4456-a08e-e656bce0ea20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape:  torch.Size([2, 5])\n",
            "K shape:  torch.Size([2, 3, 5])\n",
            "V shape:  torch.Size([2, 3, 5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-query attention"
      ],
      "metadata": {
        "id": "VpH4hRhPOOgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MultiqueryAttention(X, M, mask, W_query, W_key, W_value, P_o):\n",
        "    \"\"\"\n",
        "    Multi-Query Attention.\n",
        "    Args:\n",
        "    X: a tensor with shape [b, n, d]\n",
        "    M: a tensor with shape [b, m, d]\n",
        "    mask: a tensor with shape [b, h, n, m]\n",
        "    W_query: a tensor with shape [h, d, k]\n",
        "    W_key: a tensor with shape [d, k]\n",
        "    P_v: a tensor with shape [d, v]\n",
        "    P_o: a tensor with shape [h, d, v]\n",
        "    Returns:\n",
        "    Y: a tensor with shape [b, n, d]\n",
        "    \"\"\"\n",
        "    scaling_factor = W_key.shape[1]**0.5\n",
        "\n",
        "    # Apply projections using einsum\n",
        "    Q = torch.einsum('bnd,hdk->bhnk', X, W_query)\n",
        "    print(\"Q shape: \",Q.shape)\n",
        "    K = torch.einsum('bmd,dk->bmk', M, W_key)\n",
        "    print(\"K shape: \",K.shape)\n",
        "    V = torch.einsum('bmd,dv->bmv', M, W_value)\n",
        "    print(\"V shape: \",V.shape)\n",
        "    attn_scores = torch.einsum('bhnk,bmk->bhnm', Q, K) / scaling_factor\n",
        "    attn_weights = F.softmax(attn_scores + mask, dim=-1)\n",
        "\n",
        "    O = torch.einsum('bhnm,bmv->bhnv', attn_weights, V)\n",
        "    Y = torch.einsum('bhnv,hdv->bnd', O, P_o)\n",
        "    return Y"
      ],
      "metadata": {
        "id": "C7wgvwHIUvYz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running multi-query attention with parameters"
      ],
      "metadata": {
        "id": "VA8wDAKxOWIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup parameters\n",
        "d, m, h, k, v = 4, 3, 2, 5, 5  # dimensions for d: vector dimension, m: number of keys/values, h: number of heads, k and v: key/value dimensions\n",
        "b, n = 2, 3  # Batch size, number of queries\n",
        "\n",
        "# Input tensors\n",
        "X = torch.randn(b, n, d)\n",
        "M = torch.randn(b, m, d)\n",
        "mask = torch.zeros(b, h, n, m)  # Attention mask\n",
        "W_query = torch.randn(h, d, k)\n",
        "W_key = torch.randn(d, k)\n",
        "W_value = torch.randn(d, v)\n",
        "P_o = torch.randn(h, d, v)\n",
        "\n",
        "# Call the corrected function\n",
        "y = MultiqueryAttention(X, M, mask, W_query, W_key, W_value, P_o)\n",
        "\n",
        "y.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6FdKT_cOZYf",
        "outputId": "e203e345-f1b9-4d7d-b35e-8d030c6c0e00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape:  torch.Size([2, 2, 3, 5])\n",
            "K shape:  torch.Size([2, 3, 5])\n",
            "V shape:  torch.Size([2, 3, 5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross attention"
      ],
      "metadata": {
        "id": "nqPwA6YbOwNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CrossAttention(x_1, x_2, W_query, W_key, W_value):\n",
        "    \"\"\"\n",
        "    Cross-Attention mechanism.\n",
        "    Args:\n",
        "    x_1: a tensor with shape [b1, d_in] where b1 is the batch size for x_1\n",
        "    x_2: a tensor with shape [b2, d_in] where b2 is the batch size for x_2\n",
        "    W_query: a tensor with shape [d_in, d_out_kq]\n",
        "    W_key: a tensor with shape [d_in, d_out_kq]\n",
        "    W_value: a tensor with shape [d_in, d_out_v]\n",
        "    Returns:\n",
        "    Y: a tensor with shape [b1, d_out_v] representing the weighted sum of values based on the attention.\n",
        "    \"\"\"\n",
        "    scaling_factor = W_query.shape[1]**0.5\n",
        "\n",
        "    # Obtain queries, keys, and values from x_1 and x_2\n",
        "    Q = torch.einsum('bd,dk->bk', x_1, W_query) # Shape [b1, d_out_kq]\n",
        "    K = torch.einsum('bd,dk->bk', x_2, W_key)  # Shape[b2, d_out_kq]\n",
        "    V = torch.einsum('bd,dv->bv', x_2, W_value)  # Shape [b2, d_out_v]\n",
        "\n",
        "    # Compute attention scores and weights\n",
        "    attn_scores = torch.einsum('bk,mk->bm', Q, K) # Shape [b1, b2]\n",
        "    attn_weights = F.softmax(attn_scores / scaling_factor, dim=-1)  # Softmax over n2 dimension\n",
        "\n",
        "    # Apply attention weights to the values\n",
        "    Y = torch.einsum('bm,mv->bv', attn_weights, V)  # [n1, d_out_v]\n",
        "\n",
        "    return Y\n"
      ],
      "metadata": {
        "id": "7qfQK1XFWWX4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running cross attention with parameters"
      ],
      "metadata": {
        "id": "YnXtv3a7ObcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_in, d_out_kq, d_out_v, b1, b2 = 4, 5, 6, 2, 3\n",
        "x_1 = torch.randn(b1, d_in)\n",
        "x_2 = torch.randn(b2, d_in)\n",
        "W_query = torch.randn(d_in, d_out_kq)\n",
        "W_key = torch.randn(d_in, d_out_kq)\n",
        "W_value = torch.randn(d_in, d_out_v)\n",
        "\n",
        "# Call CrossAttention function\n",
        "Y = CrossAttention(x_1, x_2, W_query, W_key, W_value)\n",
        "print(\"Output shape: \", Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJgrRc5eyx6n",
        "outputId": "325bd434-de10-44d7-8671-26fe95bee84f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape:  torch.Size([2, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grouped-query attention"
      ],
      "metadata": {
        "id": "GmPWRc0bO4KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GroupedQueryAttention(Q, K, V, num_heads, group_size):\n",
        "    batch_size, seq_len, embed_dim = Q.shape\n",
        "    scaling_factor = (embed_dim // num_heads) ** 0.5\n",
        "\n",
        "    # Reshape Q, K, V for grouped multi-head attention computation\n",
        "    Q = rearrange(Q, 'b s (h d) -> (b h) s d', h=num_heads)\n",
        "    K = rearrange(K, 'b s (h d) -> (b h) s d', h=num_heads)\n",
        "    V = rearrange(V, 'b s (h d) -> (b h) s d', h=num_heads)\n",
        "\n",
        "    # Compute attention scores using scaled dot-product attention\n",
        "    attn_scores = torch.einsum('bid,bjd->bij', Q, K) / scaling_factor\n",
        "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # Apply attention weights to the values\n",
        "    attn_output = torch.einsum('bij,bjd->bid', attn_weights, V)\n",
        "\n",
        "    # Reshape back to the original dimensions\n",
        "    y = rearrange(attn_output, '(b h) s d -> b s (h d)', b=batch_size, h=num_heads)\n",
        "\n",
        "    return Y\n",
        "\n"
      ],
      "metadata": {
        "id": "7WAiRTm2yKu4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running multi-head attention with parameters"
      ],
      "metadata": {
        "id": "Pyz5pTCxOldo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dimensions and data\n",
        "batch_size = 2\n",
        "seq_len = 256\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "group_size = 2  # This value is now not directly used but is implied in the num_heads.\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len, embed_dim)\n",
        "K = torch.randn(batch_size, seq_len, embed_dim)\n",
        "V = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Call function\n",
        "output = GroupedQueryAttention(Q, K, V, num_heads, group_size)\n",
        "print(\"Output shape: \", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQP9MfCAOnbI",
        "outputId": "adc32db3-214b-4d6a-e2b3-efadab06e61e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape:  torch.Size([2, 6])\n"
          ]
        }
      ]
    }
  ]
}