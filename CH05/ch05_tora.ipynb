{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Open notebook in:\n",
        "| Colab                                 \n",
        ":-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nicolepcx/transformers-the-definitive-guide/blob/master/CH05/ch05_tora.ipynb)                                             "
      ],
      "metadata": {
        "id": "l1PBAUIly_8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this Notebook\n",
        "\n",
        "This notebook demonstrates how to generate **trajectory-guided videos** using **Tora**, a trajectory-oriented diffusion framework developed by Alibaba's Research & Intelligence Computing team. Tora extends conventional text-to-video models by introducing **spatial control via 2D trajectory prompts**, enabling users to influence the motion path of generated objects in video clips.\n",
        "\n",
        "### Steps Included:\n",
        "\n",
        "1. **Repository Setup and Environment Configuration**:\n",
        "   The Tora GitHub repository is cloned and the working directory is changed to the `sat/` subdirectory where the codebase resides. Required dependencies are installed, including `huggingface_hub` with fast transfer enabled to streamline model checkpoint downloads.\n",
        "\n",
        "2. **Model Checkpoint Download**:\n",
        "   The pretrained weights for **CogVideoX-5B** fine-tuned on Tora (a sparse trajectory-conditioned diffusion model) are downloaded directly from the Hugging Face Hub. These checkpoints are stored in the local `ckpts/tora/t2v` directory.\n",
        "\n",
        "3. **Inference with Trajectory Guidance**:\n",
        "   The main generation script `sample_video.py` is executed using `torchrun` with one GPU. It reads:\n",
        "\n",
        "   * A **text prompt** file that defines the visual concept (e.g., \"a drone flying over a forest\").\n",
        "   * A **trajectory file** (`trajs/coaster.txt`) that defines a 2D motion path.\n",
        "     The script generates videos consistent with both the text and trajectory prompt and stores them in the `samples` directory.\n",
        "\n",
        "4. **Interactive App Launch**:\n",
        "   A Gradio-based web interface is started using `app.py`, allowing users to upload their own prompts and trajectories to interactively generate videos without modifying the source code.\n",
        "\n",
        "### Highlights:\n",
        "\n",
        "* Combines **language-based semantics** with **trajectory control**, making video generation more precise and expressive.\n",
        "* Enables experimentation with **sparse trajectory inputs**, offering better generalization and fewer constraints than dense trajectory supervision.\n",
        "* Supports **multi-modal input**, integrating language and spatial motion cues into the video generation process.\n"
      ],
      "metadata": {
        "id": "tCXwwL_cyq8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "AQhK-FGYk4AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone this repository.\n",
        "!git clone https://github.com/Nicolepcx/Tora\n",
        "%cd Tora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4D-E19jQr45",
        "outputId": "ae0727c2-0af8-42e5-8627-c17ab9bb1c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Tora'...\n",
            "remote: Enumerating objects: 710, done.\u001b[K\n",
            "remote: Counting objects: 100% (710/710), done.\u001b[K\n",
            "remote: Compressing objects: 100% (536/536), done.\u001b[K\n",
            "remote: Total 710 (delta 204), reused 624 (delta 143), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (710/710), 4.85 MiB | 20.67 MiB/s, done.\n",
            "Resolving deltas: 100% (204/204), done.\n",
            "/content/Tora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otx6pXTsHHIP",
        "outputId": "78720477-91ba-49f1-ee2a-e9f4f073eddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CogVideoX_LICENSE  LICENSE  pyproject.toml  sat\n",
            "diffusers-version  modules  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd sat/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmGklZGPJert",
        "outputId": "1532d3cf-b5d5-466b-ade4-117d872e5e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Tora/sat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VjnHLYiUJKiL",
        "outputId": "42644c45-a96b-4b35-f8a6-8ffc217bf36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SwissArmyTransformer==0.4.12 (from -r requirements.txt (line 1))\n",
            "  Downloading SwissArmyTransformer-0.4.12-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting pytorch_lightning==2.3.3 (from -r requirements.txt (line 2))\n",
            "  Downloading pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting kornia==0.7.3 (from -r requirements.txt (line 3))\n",
            "  Downloading kornia-0.7.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting beartype==0.18.5 (from -r requirements.txt (line 4))\n",
            "  Downloading beartype-0.18.5-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting fsspec==2024.5.0 (from -r requirements.txt (line 5))\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting decord==0.6.0 (from -r requirements.txt (line 6))\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Collecting deepspeed==0.15.1 (from -r requirements.txt (line 7))\n",
            "  Downloading deepspeed-0.15.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av==13.1.0 (from -r requirements.txt (line 8))\n",
            "  Downloading av-13.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting gradio==3.50.2 (from -r requirements.txt (line 9))\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (0.2.0)\n",
            "Collecting tensorboardX (from SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (2.14.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (4.52.2)\n",
            "Collecting cpm-kernels (from SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (0.8.1)\n",
            "Collecting boto3 (from SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading boto3-1.38.26-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting webdataset (from SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (6.0.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning==2.3.3->-r requirements.txt (line 2))\n",
            "  Downloading torchmetrics-1.7.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (4.13.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning==2.3.3->-r requirements.txt (line 2))\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting kornia-rs>=0.1.0 (from kornia==0.7.3->-r requirements.txt (line 3))\n",
            "  Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting hjson (from deepspeed==0.15.1->-r requirements.txt (line 7))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ninja (from deepspeed==0.15.1->-r requirements.txt (line 7))\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.15.1->-r requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.15.1->-r requirements.txt (line 7)) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.15.1->-r requirements.txt (line 7)) (2.11.4)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.15.1->-r requirements.txt (line 7)) (12.575.51)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.50.2->-r requirements.txt (line 9)) (5.5.0)\n",
            "Collecting fastapi (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==0.6.1 (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from gradio==3.50.2->-r requirements.txt (line 9)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.50.2->-r requirements.txt (line 9)) (0.31.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==3.50.2->-r requirements.txt (line 9)) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.50.2->-r requirements.txt (line 9)) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.50.2->-r requirements.txt (line 9)) (3.10.0)\n",
            "Collecting numpy>=1.17.2 (from pytorch_lightning==2.3.3->-r requirements.txt (line 2))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.50.2->-r requirements.txt (line 9)) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.50.2->-r requirements.txt (line 9)) (2.2.2)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pydub (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.50.2->-r requirements.txt (line 9)) (2.32.3)\n",
            "Collecting semantic-version~=2.0 (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 9)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 9)) (1.40.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (3.11.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.14.0->gradio==3.50.2->-r requirements.txt (line 9)) (3.18.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.50.2->-r requirements.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.50.2->-r requirements.txt (line 9)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.50.2->-r requirements.txt (line 9)) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.50.2->-r requirements.txt (line 9)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.50.2->-r requirements.txt (line 9)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.50.2->-r requirements.txt (line 9)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==3.50.2->-r requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==3.50.2->-r requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed==0.15.1->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed==0.15.1->-r requirements.txt (line 7)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed==0.15.1->-r requirements.txt (line 7)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.0->gradio==3.50.2->-r requirements.txt (line 9)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.0->gradio==3.50.2->-r requirements.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.0->gradio==3.50.2->-r requirements.txt (line 9)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.0->gradio==3.50.2->-r requirements.txt (line 9)) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.14.0->gradio==3.50.2->-r requirements.txt (line 9)) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.14.0->gradio==3.50.2->-r requirements.txt (line 9)) (0.16.0)\n",
            "Collecting botocore<1.39.0,>=1.38.26 (from boto3->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading botocore-1.38.26-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (0.70.15)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi->gradio==3.50.2->-r requirements.txt (line 9))\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.50.2->-r requirements.txt (line 9)) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.50.2->-r requirements.txt (line 9)) (1.0.9)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (5.29.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1)) (0.5.3)\n",
            "Collecting braceexpand (from webdataset->SwissArmyTransformer==0.4.12->-r requirements.txt (line 1))\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r requirements.txt (line 2)) (1.20.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 9)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 9)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 9)) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.50.2->-r requirements.txt (line 9)) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->gradio==3.50.2->-r requirements.txt (line 9)) (1.3.1)\n",
            "Downloading SwissArmyTransformer-0.4.12-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.3.3-py3-none-any.whl (812 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.7.3-py2.py3-none-any.whl (833 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m833.3/833.3 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.18.5-py3-none-any.whl (917 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.8/917.8 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-13.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.0/34.0 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/27/94/3266821f65b92b3138631e9c8e7fe1fb513804ac934485a8d05776e1dd43/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.2-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.38.26-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdataset-0.2.111-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.38.26-py3-none-any.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.15.1-py3-none-any.whl size=1483859 sha256=104798b3db1983357a6dde251d1f7d8b45b136b854ded666fad9428186d5c72c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/77/1f/c1727c979b4d8a9d0c78c23ab0f996e779b24e36c4fcfe93b3\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: pydub, hjson, cpm-kernels, braceexpand, websockets, uvicorn, semantic-version, python-multipart, pillow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, markupsafe, lightning-utilities, kornia-rs, jmespath, fsspec, ffmpy, beartype, av, aiofiles, webdataset, tensorboardX, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, decord, botocore, s3transfer, nvidia-cusolver-cu12, gradio-client, fastapi, boto3, torchmetrics, kornia, gradio, deepspeed, SwissArmyTransformer, pytorch_lightning\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.7.4 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "google-genai 1.16.1 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.5.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "yfinance 0.2.61 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SwissArmyTransformer-0.4.12 aiofiles-23.2.1 av-13.1.0 beartype-0.18.5 boto3-1.38.26 botocore-1.38.26 braceexpand-0.1.7 cpm-kernels-1.0.11 decord-0.6.0 deepspeed-0.15.1 fastapi-0.115.12 ffmpy-0.5.0 fsspec-2024.5.0 gradio-3.50.2 gradio-client-0.6.1 hjson-3.1.0 jmespath-1.0.1 kornia-0.7.3 kornia-rs-0.1.9 lightning-utilities-0.14.3 markupsafe-2.1.5 ninja-1.11.1.4 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pillow-10.4.0 pydub-0.25.1 python-multipart-0.0.20 pytorch_lightning-2.3.3 s3transfer-0.13.0 semantic-version-2.10.0 starlette-0.46.2 tensorboardX-2.6.2.2 torchmetrics-1.7.2 uvicorn-0.34.2 webdataset-0.2.111 websockets-11.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "cb36b88a2acb4209887836c8851691b0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install modelscope -q\n",
        "!pip install \"huggingface_hub[hf_transfer]\"\n",
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Alibaba-Research-Intelligence-Computing/Tora --local-dir ckpts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83JjP-SRKSWn",
        "outputId": "31d9bd36-ae1f-4165-ff07-ae52f2272520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (0.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]) (2024.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]) (4.13.2)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]) (0.1.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_transfer]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_transfer]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_transfer]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_transfer]) (2025.4.26)\n",
            "Downloading '.gitattributes' to 'ckpts/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 11.9MB/s]\n",
            "Download complete. Moving file to ckpts/.gitattributes\n",
            "Downloading 'CogVideoX_LICENSE' to 'ckpts/.cache/huggingface/download/k5CF3exemdNS-lSlKN72gCTS5nA=.188f301bad0622302715b6caac6ea934414e77fa.incomplete'\n",
            "CogVideoX_LICENSE: 100% 5.70k/5.70k [00:00<00:00, 29.8MB/s]\n",
            "Download complete. Moving file to ckpts/CogVideoX_LICENSE\n",
            "Downloading 'LICENSE' to 'ckpts/.cache/huggingface/download/DhCjcNQuMpl4FL346qr3tvNUCgY=.261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64.incomplete'\n",
            "LICENSE: 100% 11.4k/11.4k [00:00<00:00, 48.7MB/s]\n",
            "Download complete. Moving file to ckpts/LICENSE\n",
            "Downloading 'README.md' to 'ckpts/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.b945edd8c6d118ef9f1691414b9beb5a7fc8be6a.incomplete'\n",
            "README.md: 100% 9.20k/9.20k [00:00<00:00, 36.9MB/s]\n",
            "Download complete. Moving file to ckpts/README.md\n",
            "Downloading 'icon.jpg' to 'ckpts/.cache/huggingface/download/PbW3jBxJBZ3NKrjEWyj4AOhM7Co=.3b8f5cb912c9c8fc5035574d78f9b3a9c18b707c.incomplete'\n",
            "icon.jpg: 100% 36.8k/36.8k [00:00<00:00, 35.9MB/s]\n",
            "Download complete. Moving file to ckpts/icon.jpg\n",
            "Downloading 't5-v1_1-xxl/added_tokens.json' to 'ckpts/.cache/huggingface/download/t5-v1_1-xxl/SeqzFlf9ZNZ3or_wZAOIdsM3Yxw=.3f5132007c4fcf42b75b65c8b6aa49c7098bcdf4.incomplete'\n",
            "added_tokens.json: 100% 2.59k/2.59k [00:00<00:00, 18.9MB/s]\n",
            "Download complete. Moving file to ckpts/t5-v1_1-xxl/added_tokens.json\n",
            "Downloading 't5-v1_1-xxl/config.json' to 'ckpts/.cache/huggingface/download/t5-v1_1-xxl/8_PA_wEVGiVa2goH2H4KQOQpvVY=.9bb422ed546933cdd114d6f86f08008317aa51ae.incomplete'\n",
            "config.json: 100% 783/783 [00:00<00:00, 5.69MB/s]\n",
            "Download complete. Moving file to ckpts/t5-v1_1-xxl/config.json\n",
            "Downloading 't5-v1_1-xxl/model-00001-of-00002.safetensors' to 'ckpts/.cache/huggingface/download/t5-v1_1-xxl/aoe4E07IMh7reFyUkVoVk040mQk=.4f2751ceeb2a96edd693e539dc5d6bba0b8d3814f49a9b3798403a0cec4b2e3d.incomplete'\n",
            "model-00001-of-00002.safetensors: 100% 4.99G/4.99G [00:13<00:00, 379MB/s]\n",
            "Download complete. Moving file to ckpts/t5-v1_1-xxl/model-00001-of-00002.safetensors\n",
            "Downloading 't5-v1_1-xxl/model-00002-of-00002.safetensors' to 'ckpts/.cache/huggingface/download/t5-v1_1-xxl/Dr_lZJDwE1cnGAQMwA77jJEQIk8=.f63154532130422309532ff56f11945fbea8266c958e3133e8e5aef85c6293c7.incomplete'\n",
            "model-00002-of-00002.safetensors: 100% 4.53G/4.53G [00:11<00:00, 385MB/s]\n",
            "Download complete. Moving file to ckpts/t5-v1_1-xxl/model-00002-of-00002.safetensors\n",
            "Downloading 't5-v1_1-xxl/model.safetensors.index.json' to 'ckpts/.cache/huggingface/download/t5-v1_1-xxl/yVzAsSxRSINSz-tQbpx-TLpfkLU=.c8728bc3dca59d2616a2a594cbac3ddb9eb77d5b.incomplete'\n",
            "model.safetensors.index.json: 100% 19.9k/19.9k [00:00<00:00, 91.5MB/s]\n",
            "Download complete. Moving file to ckpts/t5-v1_1-xxl/model.safetensors.index.json\n",
            "Downloading 't5-v1_1-xxl/special_tokens_map.json' to 'ckpts/.cache/huggingface/download/t5-v1_1-xxl/ahkChHUJFxEmOdq5GDFEmerRzCY=.17ade346a1042cbe0c1436f5bedcbd85c099d582.incomplete'\n",
            "special_tokens_map.json: 100% 2.54k/2.54k [00:00<00:00, 19.7MB/s]\n",
            "Download complete. Moving file to ckpts/t5-v1_1-xxl/special_tokens_map.json\n",
            "Downloading 't5-v1_1-xxl/spiece.model' to 'ckpts/.cache/huggingface/download/t5-v1_1-xxl/vj8E1loknrCNPSP8nWJC234Bff4=.d60acb128cf7b7f2536e8f38a5b18a05535c9e14c7a355904270e15b0945ea86.incomplete'\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 72.8MB/s]\n",
            "Download complete. Moving file to ckpts/t5-v1_1-xxl/spiece.model\n",
            "Downloading 't5-v1_1-xxl/tokenizer_config.json' to 'ckpts/.cache/huggingface/download/t5-v1_1-xxl/vzaExXFZNBay89bvlQv-ZcI6BTg=.161715af5ee99558c9fcce7b31d3d547a72c349b.incomplete'\n",
            "tokenizer_config.json: 100% 20.6k/20.6k [00:00<00:00, 82.0MB/s]\n",
            "Download complete. Moving file to ckpts/t5-v1_1-xxl/tokenizer_config.json\n",
            "Downloading 'tora/i2v/mp_rank_00_model_states.pt' to 'ckpts/.cache/huggingface/download/tora/i2v/s4QcvrZlWtBl2tdvN4z_1ngfd8U=.d86512e34561fd0138dc4614a59f9ea3e4bbf4dc4f660a76b505caa0de92abc9.incomplete'\n",
            "mp_rank_00_model_states.pt: 100% 22.6G/22.6G [01:00<00:00, 375MB/s]\n",
            "Download complete. Moving file to ckpts/tora/i2v/mp_rank_00_model_states.pt\n",
            "Downloading 'tora/t2v/mp_rank_00_model_states.pt' to 'ckpts/.cache/huggingface/download/tora/t2v/s4QcvrZlWtBl2tdvN4z_1ngfd8U=.a3a1ccd758af91e91c5d7ae04a9da0f96df415f9ab98631ddad58354001b18e8.incomplete'\n",
            "mp_rank_00_model_states.pt: 100% 22.5G/22.5G [01:02<00:00, 360MB/s]\n",
            "Download complete. Moving file to ckpts/tora/t2v/mp_rank_00_model_states.pt\n",
            "Downloading 'vae/3d-vae.pt' to 'ckpts/.cache/huggingface/download/vae/uE2-Y9SM9rbmphTrzCqbO3Bd1BM=.cdf2683a98192fc35ebb1f86ff0bfd620eb0f8905efa4e8eb818af759e2bc418.incomplete'\n",
            "3d-vae.pt: 100% 1.18G/1.18G [00:05<00:00, 211MB/s] \n",
            "Download complete. Moving file to ckpts/vae/3d-vae.pt\n",
            "/content/Tora/sat/ckpts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run inference"
      ],
      "metadata": {
        "id": "j8WDx3M2wZOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!N_GPU=1 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True torchrun --standalone --nproc_per_node=1 sample_video.py --base configs/tora/model/cogvideox_5b_tora.yaml configs/tora/inference_sparse.yaml --load ckpts/tora/t2v --output-dir samples --point_path trajs/coaster.txt --input-file assets/text/t2v/examples.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfI_xEYSGtky",
        "outputId": "e862ec68-c521-4c85-f1ca-418ea20b9cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-30 03:58:01,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2025-05-30 03:58:09.518104: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-30 03:58:10.226102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748577490.552699    5950 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748577490.637620    5950 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-30 03:58:11.138710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/Tora/sat/sgm/modules/encoders/modules.py:246: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  if model_dir is not \"google/t5-v1_1-xxl\":\n",
            "/usr/local/lib/python3.11/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
            "no module 'xformers'. Processing without...\n",
            "no module 'xformers'. Processing without...\n",
            "[2025-05-30 03:58:18,795] [WARNING] No training data specified\n",
            "WARNING:sat:No training data specified\n",
            "[2025-05-30 03:58:18,795] [WARNING] No train_iters (recommended) or epochs specified, use default 10k iters.\n",
            "WARNING:sat:No train_iters (recommended) or epochs specified, use default 10k iters.\n",
            "[2025-05-30 03:58:18,795] [INFO] using world size: 1\n",
            "INFO:sat:using world size: 1\n",
            "[W530 03:58:18.694775089 Utils.hpp:165] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
            "[2025-05-30 03:58:18,809] [INFO] [RANK 0] > initializing model parallel with size 1\n",
            "INFO:sat:[RANK 0] > initializing model parallel with size 1\n",
            "[2025-05-30 03:58:18,823] [INFO] [RANK 0] args:\n",
            "{ 'base': [ 'configs/tora/model/cogvideox_5b_tora.yaml',\n",
            "            'configs/tora/inference_sparse.yaml'],\n",
            "  'batch_from_same_dataset': False,\n",
            "  'batch_size': 1,\n",
            "  'bf16': True,\n",
            "  'block_size': 10000,\n",
            "  'checkpoint_activations': False,\n",
            "  'checkpoint_num_layers': 1,\n",
            "  'checkpoint_skip_layers': 0,\n",
            "  'cuda': True,\n",
            "  'debug': False,\n",
            "  'deepscale': False,\n",
            "  'deepscale_config': None,\n",
            "  'deepspeed': False,\n",
            "  'deepspeed_config': None,\n",
            "  'device': 0,\n",
            "  'distributed_backend': 'nccl',\n",
            "  'do_fuse_object_features': False,\n",
            "  'epochs': None,\n",
            "  'eval_batch_size': None,\n",
            "  'eval_interval': None,\n",
            "  'eval_iters': 100,\n",
            "  'exit_interval': None,\n",
            "  'experiment_name': 'MyModel',\n",
            "  'final_size': 2048,\n",
            "  'flow_from_prompt': False,\n",
            "  'flow_path': None,\n",
            "  'force_inference': True,\n",
            "  'force_pretrain': False,\n",
            "  'force_train': False,\n",
            "  'fp16': False,\n",
            "  'gradient_accumulation_steps': 1,\n",
            "  'grid_num_rows': 1,\n",
            "  'image2video': False,\n",
            "  'img_dir': 'cli',\n",
            "  'input_dir': None,\n",
            "  'input_file': 'assets/text/t2v/examples.txt',\n",
            "  'input_type': 'txt',\n",
            "  'iterable_dataset': False,\n",
            "  'iterable_dataset_eval': '',\n",
            "  'latent_channels': 16,\n",
            "  'lcm_steps': None,\n",
            "  'load': 'ckpts/tora/t2v',\n",
            "  'local_rank': 0,\n",
            "  'log_image': True,\n",
            "  'log_interval': 50,\n",
            "  'lr': 0.0001,\n",
            "  'lr_decay_iters': None,\n",
            "  'lr_decay_ratio': 0.1,\n",
            "  'lr_decay_style': 'linear',\n",
            "  'master_ip': '67dd712ff59c',\n",
            "  'master_port': '35687',\n",
            "  'mode': 'inference',\n",
            "  'model_config': {'scale_factor': 0.7, 'disable_first_stage_autocast': True, 'log_keys': ['txt'], 'trainable_modules': ['fuser', 'traj_extractor'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 42, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 3072, 'adm_in_channels': 256, 'num_attention_heads': 48, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Rotary3DPositionEmbeddingMixin', 'params': {'hidden_size_head': 64, 'text_length': 226}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': 'ckpts/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': 'ckpts/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}},\n",
            "  'model_parallel_size': 1,\n",
            "  'no_flow_injection': False,\n",
            "  'no_load_rng': False,\n",
            "  'no_save_rng': False,\n",
            "  'num_samples_per_prompt': 1,\n",
            "  'num_workers': 1,\n",
            "  'only_log_video_latents': False,\n",
            "  'only_save_latents': False,\n",
            "  'output_dir': 'samples',\n",
            "  'point_path': ['trajs/coaster.txt'],\n",
            "  'prefetch_factor': 4,\n",
            "  'profiling': -1,\n",
            "  'rank': 0,\n",
            "  'resume_dataloader': False,\n",
            "  'sample_flow': True,\n",
            "  'sampling_fps': 8,\n",
            "  'sampling_num_frames': 13,\n",
            "  'save': None,\n",
            "  'save_args': False,\n",
            "  'save_interval': 5000,\n",
            "  'sdedit': False,\n",
            "  'seed': 1234,\n",
            "  'split': '1000,1,1',\n",
            "  'strict_eval': False,\n",
            "  'summary_dir': '',\n",
            "  'test_data': None,\n",
            "  'train_data': None,\n",
            "  'train_data_weights': None,\n",
            "  'train_iters': 10000,\n",
            "  'use_raft': True,\n",
            "  'valid_data': None,\n",
            "  'vis_traj_features': False,\n",
            "  'wandb': False,\n",
            "  'wandb_project_name': 'default_project',\n",
            "  'warmup': 0.01,\n",
            "  'weight_decay': 0.01,\n",
            "  'world_size': 1,\n",
            "  'zero_stage': 0}\n",
            "INFO:sat:[RANK 0] args:\n",
            "{ 'base': [ 'configs/tora/model/cogvideox_5b_tora.yaml',\n",
            "            'configs/tora/inference_sparse.yaml'],\n",
            "  'batch_from_same_dataset': False,\n",
            "  'batch_size': 1,\n",
            "  'bf16': True,\n",
            "  'block_size': 10000,\n",
            "  'checkpoint_activations': False,\n",
            "  'checkpoint_num_layers': 1,\n",
            "  'checkpoint_skip_layers': 0,\n",
            "  'cuda': True,\n",
            "  'debug': False,\n",
            "  'deepscale': False,\n",
            "  'deepscale_config': None,\n",
            "  'deepspeed': False,\n",
            "  'deepspeed_config': None,\n",
            "  'device': 0,\n",
            "  'distributed_backend': 'nccl',\n",
            "  'do_fuse_object_features': False,\n",
            "  'epochs': None,\n",
            "  'eval_batch_size': None,\n",
            "  'eval_interval': None,\n",
            "  'eval_iters': 100,\n",
            "  'exit_interval': None,\n",
            "  'experiment_name': 'MyModel',\n",
            "  'final_size': 2048,\n",
            "  'flow_from_prompt': False,\n",
            "  'flow_path': None,\n",
            "  'force_inference': True,\n",
            "  'force_pretrain': False,\n",
            "  'force_train': False,\n",
            "  'fp16': False,\n",
            "  'gradient_accumulation_steps': 1,\n",
            "  'grid_num_rows': 1,\n",
            "  'image2video': False,\n",
            "  'img_dir': 'cli',\n",
            "  'input_dir': None,\n",
            "  'input_file': 'assets/text/t2v/examples.txt',\n",
            "  'input_type': 'txt',\n",
            "  'iterable_dataset': False,\n",
            "  'iterable_dataset_eval': '',\n",
            "  'latent_channels': 16,\n",
            "  'lcm_steps': None,\n",
            "  'load': 'ckpts/tora/t2v',\n",
            "  'local_rank': 0,\n",
            "  'log_image': True,\n",
            "  'log_interval': 50,\n",
            "  'lr': 0.0001,\n",
            "  'lr_decay_iters': None,\n",
            "  'lr_decay_ratio': 0.1,\n",
            "  'lr_decay_style': 'linear',\n",
            "  'master_ip': '67dd712ff59c',\n",
            "  'master_port': '35687',\n",
            "  'mode': 'inference',\n",
            "  'model_config': {'scale_factor': 0.7, 'disable_first_stage_autocast': True, 'log_keys': ['txt'], 'trainable_modules': ['fuser', 'traj_extractor'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 42, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 3072, 'adm_in_channels': 256, 'num_attention_heads': 48, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Rotary3DPositionEmbeddingMixin', 'params': {'hidden_size_head': 64, 'text_length': 226}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': 'ckpts/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': 'ckpts/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}},\n",
            "  'model_parallel_size': 1,\n",
            "  'no_flow_injection': False,\n",
            "  'no_load_rng': False,\n",
            "  'no_save_rng': False,\n",
            "  'num_samples_per_prompt': 1,\n",
            "  'num_workers': 1,\n",
            "  'only_log_video_latents': False,\n",
            "  'only_save_latents': False,\n",
            "  'output_dir': 'samples',\n",
            "  'point_path': ['trajs/coaster.txt'],\n",
            "  'prefetch_factor': 4,\n",
            "  'profiling': -1,\n",
            "  'rank': 0,\n",
            "  'resume_dataloader': False,\n",
            "  'sample_flow': True,\n",
            "  'sampling_fps': 8,\n",
            "  'sampling_num_frames': 13,\n",
            "  'save': None,\n",
            "  'save_args': False,\n",
            "  'save_interval': 5000,\n",
            "  'sdedit': False,\n",
            "  'seed': 1234,\n",
            "  'split': '1000,1,1',\n",
            "  'strict_eval': False,\n",
            "  'summary_dir': '',\n",
            "  'test_data': None,\n",
            "  'train_data': None,\n",
            "  'train_data_weights': None,\n",
            "  'train_iters': 10000,\n",
            "  'use_raft': True,\n",
            "  'valid_data': None,\n",
            "  'vis_traj_features': False,\n",
            "  'wandb': False,\n",
            "  'wandb_project_name': 'default_project',\n",
            "  'warmup': 0.01,\n",
            "  'weight_decay': 0.01,\n",
            "  'world_size': 1,\n",
            "  'zero_stage': 0}\n",
            "[2025-05-30 03:58:18,824] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...\n",
            "INFO:sat:[RANK 0] building SATVideoDiffusionEngine model ...\n",
            "Loading checkpoint shards: 100% 2/2 [00:51<00:00, 25.86s/it]\n",
            "Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False\n",
            "Working with z of shape (1, 16, 32, 32) = 16384 dimensions.\n",
            "Deleting key loss.logvar from state_dict.\n",
            "Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.\n",
            "Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.to_logits.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.to_logits.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.to_logits.3.weight from state_dict.\n",
            "Deleting key loss.discriminator.to_logits.3.bias from state_dict.\n",
            "Missing keys:  []\n",
            "Unexpected keys:  []\n",
            "Restored from ckpts/vae/3d-vae.pt\n",
            "[2025-05-30 04:00:04,217] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 11242726243\n",
            "INFO:sat:[RANK 0]  > number of parameters on model parallel rank 0: 11242726243\n",
            "[2025-05-30 04:00:15,710] [INFO] [RANK 0] [INFO] Loading checkpoint: ckpts/tora/t2v/mp_rank_00_model_states.pt\n",
            "INFO:sat:[RANK 0] [INFO] Loading checkpoint: ckpts/tora/t2v/mp_rank_00_model_states.pt\n",
            "[2025-05-30 04:01:45,364] [INFO] [RANK 0] [SUCCESS] Loaded checkpoint from: ckpts/tora/t2v/mp_rank_00_model_states.pt\n",
            "INFO:sat:[RANK 0] [SUCCESS] Loaded checkpoint from: ckpts/tora/t2v/mp_rank_00_model_states.pt\n",
            "rank and world_size 0 1\n",
            "0it [00:00, ?it/s]A cheerful rubber duck floats serenely in a bathtub filled with bubbles, the soft foam creating an inviting atmosphere. The bathroom setting is warm with bright tiles reflecting soft light. The camera captures playful angles, zeroing in on the duck's bright yellow color and big eyes. Sounds of water gently splashing and laughter fill the background, enhancing the joyous ambiance. This moment invites viewers to embrace nostalgia and childhood fun, evoking a sense of playfulness and relaxation.\n",
            "Point path: ['trajs/coaster.txt']\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:720: UserWarning: cuDNN cannot be used for large non-batch-splittable convolutions if the V8 API is not enabled or before cuDNN version 9.3+. Consider upgrading cuDNN and/or enabling the V8 API for better efficiency. (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:430.)\n",
            "  return F.conv3d(\n",
            "txt [497]\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: VPSDEDPMPP2MSampler\n",
            "Discretization: ZeroSNRDDPMDiscretization\n",
            "Guider: DynamicCFG\n",
            "\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   0% 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   2% 1/51 [00:05<04:32,  5.45s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   4% 2/51 [00:10<04:18,  5.27s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   6% 3/51 [00:15<04:08,  5.18s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   8% 4/51 [00:20<04:01,  5.14s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  10% 5/51 [00:25<03:55,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  12% 6/51 [00:30<03:50,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  14% 7/51 [00:36<03:44,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  16% 8/51 [00:41<03:39,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  18% 9/51 [00:46<03:34,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  20% 10/51 [00:51<03:29,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  22% 11/51 [00:56<03:24,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  24% 12/51 [01:01<03:18,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  25% 13/51 [01:06<03:14,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  27% 14/51 [01:11<03:09,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  29% 15/51 [01:16<03:04,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  31% 16/51 [01:22<02:59,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  33% 17/51 [01:27<02:53,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  35% 18/51 [01:32<02:48,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  37% 19/51 [01:37<02:43,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  39% 20/51 [01:42<02:38,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  41% 21/51 [01:47<02:33,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  43% 22/51 [01:52<02:28,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  45% 23/51 [01:57<02:23,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  47% 24/51 [02:02<02:18,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  49% 25/51 [02:08<02:13,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  51% 26/51 [02:13<02:07,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  53% 27/51 [02:18<02:02,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  55% 28/51 [02:23<01:57,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  57% 29/51 [02:28<01:52,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  59% 30/51 [02:33<01:47,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  61% 31/51 [02:38<01:42,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  63% 32/51 [02:43<01:37,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  65% 33/51 [02:49<01:32,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  67% 34/51 [02:54<01:27,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  69% 35/51 [02:59<01:22,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  71% 36/51 [03:04<01:16,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  73% 37/51 [03:09<01:11,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  75% 38/51 [03:14<01:06,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  76% 39/51 [03:19<01:01,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  78% 40/51 [03:24<00:56,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  80% 41/51 [03:30<00:51,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  82% 42/51 [03:35<00:46,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  84% 43/51 [03:40<00:40,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  86% 44/51 [03:45<00:35,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  88% 45/51 [03:50<00:30,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  90% 46/51 [03:55<00:25,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  92% 47/51 [04:00<00:20,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  94% 48/51 [04:05<00:15,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  96% 49/51 [04:11<00:10,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  98% 50/51 [04:16<00:05,  5.12s/it]\n",
            "1it [05:33, 333.21s/it]A dandelion puff sways gently in the wind, its seeds ready to take flight and spread into the world. The animation style highlights the delicate fibers of the puff, with soft, glowing light surrounding it. The background showcases a lush, green field, hinting at the beauty of nature. As the wind blows, the seeds dance and float away, creating an enchanting visual narrative. The gentle sounds of nature, alongside soft whispers of the breeze, enrich the overall ambiance. This serene scene invites viewers to embrace the moment of letting go, celebrating the cycle of life and new beginnings.\n",
            "Point path: ['trajs/coaster.txt']\n",
            "txt [594]\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: VPSDEDPMPP2MSampler\n",
            "Discretization: ZeroSNRDDPMDiscretization\n",
            "Guider: DynamicCFG\n",
            "\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   0% 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   2% 1/51 [00:04<04:09,  5.00s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   4% 2/51 [00:10<04:09,  5.09s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   6% 3/51 [00:15<04:04,  5.09s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   8% 4/51 [00:20<03:59,  5.09s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  10% 5/51 [00:25<03:54,  5.09s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  12% 6/51 [00:30<03:49,  5.09s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  14% 7/51 [00:35<03:44,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  16% 8/51 [00:40<03:39,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  18% 9/51 [00:45<03:34,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  20% 10/51 [00:50<03:29,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  22% 11/51 [00:56<03:24,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  24% 12/51 [01:01<03:19,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  25% 13/51 [01:06<03:14,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  27% 14/51 [01:11<03:09,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  29% 15/51 [01:16<03:04,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  31% 16/51 [01:21<02:58,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  33% 17/51 [01:26<02:53,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  35% 18/51 [01:31<02:48,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  37% 19/51 [01:36<02:43,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  39% 20/51 [01:42<02:38,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  41% 21/51 [01:47<02:33,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  43% 22/51 [01:52<02:28,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  45% 23/51 [01:57<02:23,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  47% 24/51 [02:02<02:18,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  49% 25/51 [02:07<02:13,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  51% 26/51 [02:12<02:07,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  53% 27/51 [02:17<02:02,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  55% 28/51 [02:23<01:57,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  57% 29/51 [02:28<01:52,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  59% 30/51 [02:33<01:47,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  61% 31/51 [02:38<01:42,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  63% 32/51 [02:43<01:37,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  65% 33/51 [02:48<01:32,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  67% 34/51 [02:53<01:26,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  69% 35/51 [02:58<01:21,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  71% 36/51 [03:03<01:16,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  73% 37/51 [03:09<01:11,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  75% 38/51 [03:14<01:06,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  76% 39/51 [03:19<01:01,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  78% 40/51 [03:24<00:56,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  80% 41/51 [03:29<00:51,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  82% 42/51 [03:34<00:46,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  84% 43/51 [03:39<00:40,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  86% 44/51 [03:44<00:35,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  88% 45/51 [03:50<00:30,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  90% 46/51 [03:55<00:25,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  92% 47/51 [04:00<00:20,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  94% 48/51 [04:05<00:15,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  96% 49/51 [04:10<00:10,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  98% 50/51 [04:15<00:05,  5.11s/it]\n",
            "2it [10:37, 315.94s/it]A golden retriever, sporting sleek black sunglasses, with its lengthy fur flowing in the breeze, sprints playfully across a rooftop terrace, recently refreshed by a light rain. The scene unfolds from a distance, the dog's energetic bounds growing larger as it approaches the camera, its tail wagging with unrestrained joy, while droplets of water glisten on the concrete behind it. The overcast sky provides a dramatic backdrop, emphasizing the vibrant golden coat of the canine as it dashes towards the viewer.\n",
            "Point path: ['trajs/coaster.txt']\n",
            "txt [511]\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: VPSDEDPMPP2MSampler\n",
            "Discretization: ZeroSNRDDPMDiscretization\n",
            "Guider: DynamicCFG\n",
            "\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   0% 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   2% 1/51 [00:05<04:10,  5.01s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   4% 2/51 [00:10<04:09,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   6% 3/51 [00:15<04:04,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   8% 4/51 [00:20<03:59,  5.09s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  10% 5/51 [00:25<03:54,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  12% 6/51 [00:30<03:49,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  14% 7/51 [00:35<03:44,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  16% 8/51 [00:40<03:39,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  18% 9/51 [00:45<03:34,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  20% 10/51 [00:51<03:29,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  22% 11/51 [00:56<03:24,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  24% 12/51 [01:01<03:19,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  25% 13/51 [01:06<03:14,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  27% 14/51 [01:11<03:09,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  29% 15/51 [01:16<03:04,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  31% 16/51 [01:21<02:59,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  33% 17/51 [01:26<02:53,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  35% 18/51 [01:31<02:48,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  37% 19/51 [01:37<02:43,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  39% 20/51 [01:42<02:38,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  41% 21/51 [01:47<02:33,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  43% 22/51 [01:52<02:28,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  45% 23/51 [01:57<02:23,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  47% 24/51 [02:02<02:18,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  49% 25/51 [02:07<02:13,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  51% 26/51 [02:12<02:07,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  53% 27/51 [02:18<02:02,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  55% 28/51 [02:23<01:57,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  57% 29/51 [02:28<01:52,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  59% 30/51 [02:33<01:47,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  61% 31/51 [02:38<01:42,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  63% 32/51 [02:43<01:37,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  65% 33/51 [02:48<01:32,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  67% 34/51 [02:53<01:27,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  69% 35/51 [02:58<01:21,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  71% 36/51 [03:04<01:16,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  73% 37/51 [03:09<01:11,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  75% 38/51 [03:14<01:06,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  76% 39/51 [03:19<01:01,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  78% 40/51 [03:24<00:56,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  80% 41/51 [03:29<00:51,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  82% 42/51 [03:34<00:46,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  84% 43/51 [03:39<00:40,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  86% 44/51 [03:45<00:35,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  88% 45/51 [03:50<00:30,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  90% 46/51 [03:55<00:25,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  92% 47/51 [04:00<00:20,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  94% 48/51 [04:05<00:15,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  96% 49/51 [04:10<00:10,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  98% 50/51 [04:15<00:05,  5.11s/it]\n",
            "3it [15:40, 310.10s/it]A squirrel gathering nuts.\n",
            "Point path: ['trajs/coaster.txt']\n",
            "txt [26]\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: VPSDEDPMPP2MSampler\n",
            "Discretization: ZeroSNRDDPMDiscretization\n",
            "Guider: DynamicCFG\n",
            "\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   0% 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   2% 1/51 [00:04<04:09,  5.00s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   4% 2/51 [00:10<04:08,  5.08s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   6% 3/51 [00:15<04:03,  5.08s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:   8% 4/51 [00:20<03:58,  5.08s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  10% 5/51 [00:25<03:53,  5.08s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  12% 6/51 [00:30<03:48,  5.09s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  14% 7/51 [00:35<03:44,  5.09s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  16% 8/51 [00:40<03:38,  5.09s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  18% 9/51 [00:45<03:34,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  20% 10/51 [00:50<03:28,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  22% 11/51 [00:55<03:24,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  24% 12/51 [01:01<03:19,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  25% 13/51 [01:06<03:13,  5.10s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  27% 14/51 [01:11<03:08,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  29% 15/51 [01:16<03:03,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  31% 16/51 [01:21<02:58,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  33% 17/51 [01:26<02:53,  5.11s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  35% 18/51 [01:31<02:48,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  37% 19/51 [01:36<02:43,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  39% 20/51 [01:42<02:38,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  41% 21/51 [01:47<02:33,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  43% 22/51 [01:52<02:28,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  45% 23/51 [01:57<02:23,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  47% 24/51 [02:02<02:18,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  49% 25/51 [02:07<02:13,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  51% 26/51 [02:12<02:07,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  53% 27/51 [02:17<02:02,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  55% 28/51 [02:22<01:57,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  57% 29/51 [02:28<01:52,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  59% 30/51 [02:33<01:47,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  61% 31/51 [02:38<01:42,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  63% 32/51 [02:43<01:37,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  65% 33/51 [02:48<01:32,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  67% 34/51 [02:53<01:27,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  69% 35/51 [02:58<01:21,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  71% 36/51 [03:03<01:16,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  73% 37/51 [03:09<01:11,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  75% 38/51 [03:14<01:06,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  76% 39/51 [03:19<01:01,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  78% 40/51 [03:24<00:56,  5.13s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  80% 41/51 [03:29<00:51,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  82% 42/51 [03:34<00:46,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  84% 43/51 [03:39<00:40,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  86% 44/51 [03:44<00:35,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  88% 45/51 [03:50<00:30,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  90% 46/51 [03:55<00:25,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  92% 47/51 [04:00<00:20,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  94% 48/51 [04:05<00:15,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  96% 49/51 [04:10<00:10,  5.12s/it]\u001b[A\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  98% 50/51 [04:15<00:05,  5.11s/it]\n",
            "4it [20:42, 310.64s/it]\n",
            "[rank0]:[W530 04:22:30.869562206 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Gradio app"
      ],
      "metadata": {
        "id": "BMfVfZ09wcSX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZUkhUyjQkZD",
        "outputId": "d68e0637-83fd-45e0-98dc-20816cab0119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
            "2025-05-30 04:29:43.568671: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-30 04:29:43.588541: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748579383.610215   14161 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748579383.616727   14161 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-30 04:29:43.640112: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "no module 'xformers'. Processing without...\n",
            "no module 'xformers'. Processing without...\n",
            "[2025-05-30 04:29:46,478] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-05-30 04:29:49,598] [WARNING] No training data specified\n",
            "WARNING:sat:No training data specified\n",
            "[2025-05-30 04:29:49,598] [WARNING] No train_iters (recommended) or epochs specified, use default 10k iters.\n",
            "WARNING:sat:No train_iters (recommended) or epochs specified, use default 10k iters.\n",
            "[2025-05-30 04:29:49,598] [INFO] using world size: 1\n",
            "INFO:sat:using world size: 1\n",
            "[W530 04:29:49.495463926 Utils.hpp:165] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n",
            "[2025-05-30 04:29:49,605] [INFO] [RANK 0] > initializing model parallel with size 1\n",
            "INFO:sat:[RANK 0] > initializing model parallel with size 1\n",
            "[2025-05-30 04:29:49,606] [INFO] [RANK 0] You didn't pass in LOCAL_WORLD_SIZE environment variable. We use the guessed LOCAL_WORLD_SIZE=1. If this is wrong, please pass the LOCAL_WORLD_SIZE manually.\n",
            "INFO:sat:[RANK 0] You didn't pass in LOCAL_WORLD_SIZE environment variable. We use the guessed LOCAL_WORLD_SIZE=1. If this is wrong, please pass the LOCAL_WORLD_SIZE manually.\n",
            "[2025-05-30 04:29:49,608] [INFO] [RANK 0] args:\n",
            "{ 'base': [ 'configs/tora/model/cogvideox_5b_tora.yaml',\n",
            "            'configs/tora/inference_sparse.yaml'],\n",
            "  'batch_from_same_dataset': False,\n",
            "  'batch_size': 1,\n",
            "  'bf16': True,\n",
            "  'block_size': 10000,\n",
            "  'checkpoint_activations': False,\n",
            "  'checkpoint_num_layers': 1,\n",
            "  'checkpoint_skip_layers': 0,\n",
            "  'cuda': True,\n",
            "  'debug': False,\n",
            "  'deepscale': False,\n",
            "  'deepscale_config': None,\n",
            "  'deepspeed': False,\n",
            "  'deepspeed_config': None,\n",
            "  'device': 0,\n",
            "  'distributed_backend': 'nccl',\n",
            "  'do_fuse_object_features': False,\n",
            "  'epochs': None,\n",
            "  'eval_batch_size': None,\n",
            "  'eval_interval': None,\n",
            "  'eval_iters': 100,\n",
            "  'exit_interval': None,\n",
            "  'experiment_name': 'MyModel',\n",
            "  'final_size': 2048,\n",
            "  'flow_from_prompt': False,\n",
            "  'flow_path': None,\n",
            "  'force_inference': True,\n",
            "  'force_pretrain': False,\n",
            "  'force_train': False,\n",
            "  'fp16': False,\n",
            "  'gradient_accumulation_steps': 1,\n",
            "  'grid_num_rows': 1,\n",
            "  'image2video': False,\n",
            "  'img_dir': 'cli',\n",
            "  'input_dir': None,\n",
            "  'input_file': 'input.txt',\n",
            "  'input_type': 'txt',\n",
            "  'iterable_dataset': False,\n",
            "  'iterable_dataset_eval': '',\n",
            "  'latent_channels': 16,\n",
            "  'lcm_steps': None,\n",
            "  'load': 'ckpts/tora/t2v',\n",
            "  'local_rank': 0,\n",
            "  'log_image': True,\n",
            "  'log_interval': 50,\n",
            "  'lr': 0.0001,\n",
            "  'lr_decay_iters': None,\n",
            "  'lr_decay_ratio': 0.1,\n",
            "  'lr_decay_style': 'linear',\n",
            "  'master_ip': 'localhost',\n",
            "  'master_port': '46549',\n",
            "  'mode': 'inference',\n",
            "  'model_config': {'scale_factor': 0.7, 'disable_first_stage_autocast': True, 'log_keys': ['txt'], 'trainable_modules': ['fuser', 'traj_extractor'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 42, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 3072, 'adm_in_channels': 256, 'num_attention_heads': 48, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Rotary3DPositionEmbeddingMixin', 'params': {'hidden_size_head': 64, 'text_length': 226}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': 'ckpts/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': 'ckpts/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}},\n",
            "  'model_parallel_size': 1,\n",
            "  'no_flow_injection': False,\n",
            "  'no_load_rng': False,\n",
            "  'no_save_rng': False,\n",
            "  'num_samples_per_prompt': 1,\n",
            "  'num_workers': 1,\n",
            "  'only_log_video_latents': False,\n",
            "  'only_save_latents': False,\n",
            "  'output_dir': 'samples',\n",
            "  'point_path': None,\n",
            "  'prefetch_factor': 4,\n",
            "  'profiling': -1,\n",
            "  'rank': 0,\n",
            "  'resume_dataloader': False,\n",
            "  'sample_flow': True,\n",
            "  'sampling_fps': 8,\n",
            "  'sampling_num_frames': 13,\n",
            "  'save': None,\n",
            "  'save_args': False,\n",
            "  'save_interval': 5000,\n",
            "  'sdedit': False,\n",
            "  'seed': 1234,\n",
            "  'split': '1000,1,1',\n",
            "  'strict_eval': False,\n",
            "  'summary_dir': '',\n",
            "  'test_data': None,\n",
            "  'train_data': None,\n",
            "  'train_data_weights': None,\n",
            "  'train_iters': 10000,\n",
            "  'use_raft': True,\n",
            "  'valid_data': None,\n",
            "  'vis_traj_features': False,\n",
            "  'wandb': False,\n",
            "  'wandb_project_name': 'default_project',\n",
            "  'warmup': 0.01,\n",
            "  'weight_decay': 0.01,\n",
            "  'world_size': 1,\n",
            "  'zero_stage': 0}\n",
            "INFO:sat:[RANK 0] args:\n",
            "{ 'base': [ 'configs/tora/model/cogvideox_5b_tora.yaml',\n",
            "            'configs/tora/inference_sparse.yaml'],\n",
            "  'batch_from_same_dataset': False,\n",
            "  'batch_size': 1,\n",
            "  'bf16': True,\n",
            "  'block_size': 10000,\n",
            "  'checkpoint_activations': False,\n",
            "  'checkpoint_num_layers': 1,\n",
            "  'checkpoint_skip_layers': 0,\n",
            "  'cuda': True,\n",
            "  'debug': False,\n",
            "  'deepscale': False,\n",
            "  'deepscale_config': None,\n",
            "  'deepspeed': False,\n",
            "  'deepspeed_config': None,\n",
            "  'device': 0,\n",
            "  'distributed_backend': 'nccl',\n",
            "  'do_fuse_object_features': False,\n",
            "  'epochs': None,\n",
            "  'eval_batch_size': None,\n",
            "  'eval_interval': None,\n",
            "  'eval_iters': 100,\n",
            "  'exit_interval': None,\n",
            "  'experiment_name': 'MyModel',\n",
            "  'final_size': 2048,\n",
            "  'flow_from_prompt': False,\n",
            "  'flow_path': None,\n",
            "  'force_inference': True,\n",
            "  'force_pretrain': False,\n",
            "  'force_train': False,\n",
            "  'fp16': False,\n",
            "  'gradient_accumulation_steps': 1,\n",
            "  'grid_num_rows': 1,\n",
            "  'image2video': False,\n",
            "  'img_dir': 'cli',\n",
            "  'input_dir': None,\n",
            "  'input_file': 'input.txt',\n",
            "  'input_type': 'txt',\n",
            "  'iterable_dataset': False,\n",
            "  'iterable_dataset_eval': '',\n",
            "  'latent_channels': 16,\n",
            "  'lcm_steps': None,\n",
            "  'load': 'ckpts/tora/t2v',\n",
            "  'local_rank': 0,\n",
            "  'log_image': True,\n",
            "  'log_interval': 50,\n",
            "  'lr': 0.0001,\n",
            "  'lr_decay_iters': None,\n",
            "  'lr_decay_ratio': 0.1,\n",
            "  'lr_decay_style': 'linear',\n",
            "  'master_ip': 'localhost',\n",
            "  'master_port': '46549',\n",
            "  'mode': 'inference',\n",
            "  'model_config': {'scale_factor': 0.7, 'disable_first_stage_autocast': True, 'log_keys': ['txt'], 'trainable_modules': ['fuser', 'traj_extractor'], 'denoiser_config': {'target': 'sgm.modules.diffusionmodules.denoiser.DiscreteDenoiser', 'params': {'num_idx': 1000, 'quantize_c_noise': False, 'weighting_config': {'target': 'sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting'}, 'scaling_config': {'target': 'sgm.modules.diffusionmodules.denoiser_scaling.VideoScaling'}, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}}}, 'network_config': {'target': 'dit_video_concat.DiffusionTransformer', 'params': {'time_embed_dim': 512, 'elementwise_affine': True, 'num_frames': 49, 'time_compressed_rate': 4, 'latent_width': 90, 'latent_height': 60, 'num_layers': 42, 'patch_size': 2, 'in_channels': 16, 'out_channels': 16, 'hidden_size': 3072, 'adm_in_channels': 256, 'num_attention_heads': 48, 'transformer_args': {'checkpoint_activations': True, 'vocab_size': 1, 'max_sequence_length': 64, 'layernorm_order': 'pre', 'skip_init': False, 'model_parallel_size': 1, 'is_decoder': False}, 'modules': {'pos_embed_config': {'target': 'dit_video_concat.Rotary3DPositionEmbeddingMixin', 'params': {'hidden_size_head': 64, 'text_length': 226}}, 'patch_embed_config': {'target': 'dit_video_concat.ImagePatchEmbeddingMixin', 'params': {'text_hidden_size': 4096}}, 'adaln_layer_config': {'target': 'dit_video_concat.AdaLNMixin', 'params': {'qk_ln': True}}, 'final_layer_config': {'target': 'dit_video_concat.FinalLayerMixin'}}}}, 'conditioner_config': {'target': 'sgm.modules.GeneralConditioner', 'params': {'emb_models': [{'is_trainable': False, 'input_key': 'txt', 'ucg_rate': 0.1, 'target': 'sgm.modules.encoders.modules.FrozenT5Embedder', 'params': {'model_dir': 'ckpts/t5-v1_1-xxl', 'max_length': 226}}]}}, 'first_stage_config': {'target': 'vae_modules.autoencoder.VideoAutoencoderInferenceWrapper', 'params': {'cp_size': 1, 'ckpt_path': 'ckpts/vae/3d-vae.pt', 'ignore_keys': ['loss'], 'loss_config': {'target': 'torch.nn.Identity'}, 'regularizer_config': {'target': 'vae_modules.regularizers.DiagonalGaussianRegularizer'}, 'encoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelEncoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': True}}, 'decoder_config': {'target': 'vae_modules.cp_enc_dec.ContextParallelDecoder3D', 'params': {'double_z': True, 'z_channels': 16, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 2, 4], 'attn_resolutions': [], 'num_res_blocks': 3, 'dropout': 0.0, 'gather_norm': False}}}}, 'loss_fn_config': {'target': 'sgm.modules.diffusionmodules.loss.VideoDiffusionLoss', 'params': {'offset_noise_level': 0, 'sigma_sampler_config': {'target': 'sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling', 'params': {'uniform_sampling': True, 'num_idx': 1000, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}}}}}, 'sampler_config': {'target': 'sgm.modules.diffusionmodules.sampling.VPSDEDPMPP2MSampler', 'params': {'num_steps': 50, 'verbose': True, 'discretization_config': {'target': 'sgm.modules.diffusionmodules.discretizer.ZeroSNRDDPMDiscretization', 'params': {'shift_scale': 1.0}}, 'guider_config': {'target': 'sgm.modules.diffusionmodules.guiders.DynamicCFG', 'params': {'scale': 6, 'exp': 5, 'num_steps': 50}}}}},\n",
            "  'model_parallel_size': 1,\n",
            "  'no_flow_injection': False,\n",
            "  'no_load_rng': False,\n",
            "  'no_save_rng': False,\n",
            "  'num_samples_per_prompt': 1,\n",
            "  'num_workers': 1,\n",
            "  'only_log_video_latents': False,\n",
            "  'only_save_latents': False,\n",
            "  'output_dir': 'samples',\n",
            "  'point_path': None,\n",
            "  'prefetch_factor': 4,\n",
            "  'profiling': -1,\n",
            "  'rank': 0,\n",
            "  'resume_dataloader': False,\n",
            "  'sample_flow': True,\n",
            "  'sampling_fps': 8,\n",
            "  'sampling_num_frames': 13,\n",
            "  'save': None,\n",
            "  'save_args': False,\n",
            "  'save_interval': 5000,\n",
            "  'sdedit': False,\n",
            "  'seed': 1234,\n",
            "  'split': '1000,1,1',\n",
            "  'strict_eval': False,\n",
            "  'summary_dir': '',\n",
            "  'test_data': None,\n",
            "  'train_data': None,\n",
            "  'train_data_weights': None,\n",
            "  'train_iters': 10000,\n",
            "  'use_raft': True,\n",
            "  'valid_data': None,\n",
            "  'vis_traj_features': False,\n",
            "  'wandb': False,\n",
            "  'wandb_project_name': 'default_project',\n",
            "  'warmup': 0.01,\n",
            "  'weight_decay': 0.01,\n",
            "  'world_size': 1,\n",
            "  'zero_stage': 0}\n",
            "[2025-05-30 04:29:49,609] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...\n",
            "INFO:sat:[RANK 0] building SATVideoDiffusionEngine model ...\n",
            "Loading checkpoint shards: 100% 2/2 [00:22<00:00, 11.36s/it]\n",
            "Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False\n",
            "Working with z of shape (1, 16, 32, 32) = 16384 dimensions.\n",
            "Deleting key loss.logvar from state_dict.\n",
            "Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.\n",
            "Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.\n",
            "Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.\n",
            "Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.\n",
            "Deleting key loss.discriminator.to_logits.0.weight from state_dict.\n",
            "Deleting key loss.discriminator.to_logits.0.bias from state_dict.\n",
            "Deleting key loss.discriminator.to_logits.3.weight from state_dict.\n",
            "Deleting key loss.discriminator.to_logits.3.bias from state_dict.\n",
            "Missing keys:  []\n",
            "Unexpected keys:  []\n",
            "Restored from ckpts/vae/3d-vae.pt\n",
            "[2025-05-30 04:31:11,430] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 11242726243\n",
            "INFO:sat:[RANK 0]  > number of parameters on model parallel rank 0: 11242726243\n",
            "[2025-05-30 04:31:23,082] [INFO] [RANK 0] [INFO] Loading checkpoint: ckpts/tora/t2v/mp_rank_00_model_states.pt\n",
            "INFO:sat:[RANK 0] [INFO] Loading checkpoint: ckpts/tora/t2v/mp_rank_00_model_states.pt\n",
            "[2025-05-30 04:32:42,198] [INFO] [RANK 0] [SUCCESS] Loaded checkpoint from: ckpts/tora/t2v/mp_rank_00_model_states.pt\n",
            "INFO:sat:[RANK 0] [SUCCESS] Loaded checkpoint from: ckpts/tora/t2v/mp_rank_00_model_states.pt\n",
            "******************** model loaded ********************\n",
            "Checking temporary files...\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://3f681e17cff82dabaf.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "received 1 trajectorie(s)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:720: UserWarning: cuDNN cannot be used for large non-batch-splittable convolutions if the V8 API is not enabled or before cuDNN version 9.3+. Consider upgrading cuDNN and/or enabling the V8 API for better efficiency. (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:430.)\n",
            "  return F.conv3d(\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: VPSDEDPMPP2MSampler\n",
            "Discretization: ZeroSNRDDPMDiscretization\n",
            "Guider: DynamicCFG\n",
            "Sampling with VPSDEDPMPP2MSampler for 51 steps:  98% 50/51 [04:15<00:05,  5.12s/it]\n",
            "write video success.\n",
            "['/tmp/Tora/tmpcw2u163b.mp4', '/tmp/Tora/tmprb5jvwnb.mp4']\n",
            "Keyboard interruption in main thread... closing server.\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2361, in block_thread\n",
            "[rank0]:     time.sleep(0.1)\n",
            "[rank0]: KeyboardInterrupt\n",
            "\n",
            "[rank0]: During handling of the above exception, another exception occurred:\n",
            "\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/Tora/sat/app.py\", line 1327, in <module>\n",
            "[rank0]:     main()\n",
            "[rank0]:   File \"/content/Tora/sat/app.py\", line 1290, in main\n",
            "[rank0]:     demo.queue(max_size=32).launch(share=True)\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2266, in launch\n",
            "[rank0]:     self.block_thread()\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2363, in block_thread\n",
            "[rank0]:     print(\"Keyboard interruption in main thread... closing server.\")\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/console_capture.py\", line 147, in write_with_callbacks\n",
            "[rank0]:     n = orig_write(s)\n",
            "[rank0]:         ^^^^^^^^^^^^^\n",
            "[rank0]: KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3f681e17cff82dabaf.gradio.live\n",
            "[rank0]:[W530 05:13:08.922847420 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python app.py --load ckpts/tora/t2v"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bKinkFYcqiu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}